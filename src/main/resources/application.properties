# Part 1: Configure the Ollama connection
# Point to your local Ollama instance. This disables the Ollama Dev Service.
quarkus.langchain4j.ollama.base-url=http://localhost:11434

# Specify the chat model for generating answers.
quarkus.langchain4j.ollama.chat-model.model-id=gpt-oss:20b

# Specify the model for creating embeddings.
quarkus.langchain4j.ollama.embedding-model.model-id=nomic-embed-text

# Local inference can be slower, so we increase the default timeout.
quarkus.langchain4j.ollama.timeout=600s

# Part 2: Configure the PgVector Store
# This MUST match the output dimension of the embedding model.
# The nomic-embed-text-v1.5 model supports variable dimensions up to 768.
quarkus.langchain4j.pgvector.dimension=768

# When running in dev or test mode, instruct the extension to automatically
# execute 'CREATE EXTENSION IF NOT EXISTS vector;' in the database.
quarkus.langchain4j.pgvector.register-vector-pg-extension=true

# For this tutorial, we want a clean slate on every application restart.
quarkus.langchain4j.pgvector.drop-table-first=true

# Part 3: Configure our data source location
# This is a custom property we'll use in our code to locate the knowledge base file.
rag.data.path=library_books.csv